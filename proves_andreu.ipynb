{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "import itertools\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from numpy.matlib import repmat\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B22-25', 'B22-26', 'B22-27', 'B22-28', 'B22-29', 'B22-30', 'B22-34', 'B22-35', 'B22-37', 'B22-38', 'B22-45', 'B22-46', 'B22-47', 'B22-50', 'B22-51', 'B22-52', 'B22-53', 'B22-54', 'B22-55', 'B22-56']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_cropped(folder_path, csv_path, patient_list = [], sample_size=200):\n",
    "    # Cargar el CSV con pandas\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Convertir el CSV a un diccionario para un acceso rápido\n",
    "    patient_metadata = {row['CODI']: row['DENSITAT'] for _, row in df.iterrows()}\n",
    "\n",
    "    # Inicializar la estructura de datos para almacenar los datos de los pacientes seleccionados\n",
    "    patients_data = []\n",
    "    images_list = []\n",
    "\n",
    "    # si no se proporciona una lista de pacientes, se seleccionan todos los pacientes iterando folder_path\n",
    "    if not patient_list:\n",
    "        for patient_folder in glob.glob(os.path.join(folder_path, \"*\")):\n",
    "            patient_id = os.path.basename(patient_folder).split(\"_\")[0]\n",
    "            patient_list.append(patient_id)\n",
    "    \n",
    "    # Iterar sobre cada paciente en la lista de IDs proporcionada\n",
    "    for patient_id in patient_list:\n",
    "        # Obtener carpeta del paciente\n",
    "        patient_folder = glob.glob(os.path.join(folder_path, f\"{patient_id}_*\"))[0]\n",
    "\n",
    "        # Verificar que el paciente esté en el CSV\n",
    "        if patient_id in patient_metadata:\n",
    "            # Obtener todas las imágenes .png dentro de la carpeta del paciente\n",
    "            images = glob.glob(os.path.join(patient_folder, \"*.png\"))\n",
    "            \n",
    "            # Si el paciente tiene imágenes en su carpeta\n",
    "            if images:\n",
    "                # Mezclar la lista de imágenes\n",
    "                random.shuffle(images)\n",
    "                \n",
    "                # Seleccionar una muestra de tamaño sample_size o menos si hay menos imágenes\n",
    "                images_sampled = random.sample(images, min(sample_size, len(images)))\n",
    "                \n",
    "                for image_path in images_sampled:\n",
    "                    # Cargar la imagen en formato BGR con cv2\n",
    "                    image_bgr = cv2.imread(image_path)\n",
    "                    \n",
    "                    # Convertir la imagen de BGR a RGB\n",
    "                    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    # Permutar canales\n",
    "                    image_rgb = np.transpose(image_rgb, (2, 0, 1))\n",
    "                    \n",
    "                    # Añadir la imagen a la lista de imágenes en formato RGB\n",
    "                    images_list.append(image_rgb)\n",
    "                # Binariar densidad\n",
    "                if patient_metadata[patient_id] == \"NEGATIVA\":\n",
    "                    dens = 0\n",
    "                else:\n",
    "                    dens = 1\n",
    "                \n",
    "                # Añadir la densidad a la lista de metadatos\n",
    "                patients_data.extend([dens] * len(images_sampled))\n",
    "    \n",
    "    return images_list, patients_data\n",
    "\n",
    "# Paso 2: Crear la clase Standard_Dataset\n",
    "class Standard_Dataset(data.Dataset):\n",
    "    def __init__(self, X, Y=None, transformation=None):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        self.transformation = transformation\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]\n",
    "        if self.transformation:\n",
    "            image = self.transformation(image)\n",
    "            \n",
    "        image_tensor = torch.from_numpy(image).float()\n",
    "        \n",
    "        if self.y is not None:\n",
    "            label = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "            return image_tensor, label\n",
    "        else:\n",
    "            return image_tensor\n",
    "\n",
    "# Paso 3: Cargar los datos\n",
    "folder_path = \"C:/Users/mirvi/Desktop/mii/UAB/4.1/PSIV2/detect mateicules/repte3/psiv-repte3/data/Cropped_sample\"\n",
    "csv_path = \"C:/Users/mirvi/Desktop/mii/UAB/4.1/PSIV2/detect mateicules/repte3/psiv-repte3/data/PatientDiagnosis.csv\"\n",
    "patient_list = []\n",
    "sample_size = 1\n",
    "\n",
    "# Filtrando imágenes y etiquetas para obtener solo aquellas cuya densidad es 0\n",
    "images, labels = load_cropped(folder_path, csv_path, patient_list, sample_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def init_weights_transformer(module,InitParams):\n",
    "    Type=InitParams['Type']\n",
    "    U=InitParams['U']\n",
    "    \n",
    "    if Type=='ori':\n",
    "        init_weights_transformer_ori(module)\n",
    "    elif Type=='all':\n",
    "        init_weights_transformer_all(module,U)       \n",
    "    else:\n",
    "        init_weights_transformer_exceptnorm(module,U) \n",
    "    \n",
    "def init_weights_transformer_all(module,U=[0,1]):\n",
    "    param=module.state_dict()\n",
    "   \n",
    "    for name in param.keys():\n",
    "        if name.find('norm')<0:\n",
    "           nn.init.uniform_(param[name],a=U[0],b=U[1])\n",
    "\n",
    "                \n",
    "def init_weights_transformer_exceptnorm(module,U=[0,1]):\n",
    "    param=module.state_dict()\n",
    "   \n",
    "    for name in param.keys():\n",
    "        if name.find('norm')<0:\n",
    "           if (name.find('weight')>0):\n",
    "                nn.init.uniform_(param[name],a=U[0],b=U[1])\n",
    "           elif (name.find('bias')>0):\n",
    "                nn.init.constant_(param[name], 0)\n",
    "    #        \n",
    "def init_weights_transformer_ori(module):\n",
    "    initrange = 0.1\n",
    "    nn.init.uniform_(module.encoder.weight, -initrange, initrange)\n",
    "    nn.init.zeros_(module.decoder1[0].bias)\n",
    "    nn.init.zeros_(module.decoder2.bias)\n",
    "    nn.init.uniform_(module.decoder1[0].weight, -initrange, initrange)\n",
    "    nn.init.uniform_(module.decoder2.weight, -initrange, initrange)\n",
    "    \n",
    "\n",
    "def init_weights_xavier_normal(module):\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv3d):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm3d):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LSTM):\n",
    "            for param in m.parameters():\n",
    "                if len(param.shape) >= 2:\n",
    "                    nn.init.orthogonal_(param.data)\n",
    "                else:\n",
    "                    nn.init.normal_(param.data)\n",
    "\n",
    "def init_weights_xavier_uniform(module):\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv3d):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.uniform_(m.weight, a=0, b=1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.uniform_(m.weight, a=0, b=1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm3d):\n",
    "            nn.init.uniform_(m.weight, a=0, b=1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LSTM):\n",
    "            for param in m.parameters():\n",
    "                if len(param.shape) >= 2:\n",
    "                    nn.init.orthogonal_(param.data)\n",
    "                else:\n",
    "                    nn.init.uniform_(param.data)\n",
    "\n",
    "def init_weights_kaiming_uniform(module):\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.uniform_(m.weight, a=0, b=1)\n",
    "            nn.init.constant_(m.bias, val=0.)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.uniform_(m.weight, a=0, b=1)\n",
    "            nn.init.constant_(m.bias, val=0.)\n",
    "        elif isinstance(m, nn.BatchNorm3d):\n",
    "            nn.init.uniform_(m.weight, a=0, b=1)\n",
    "            nn.init.constant_(m.bias, val=0.)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, val=0.)\n",
    "\n",
    "def init_weights_kaiming_normal(module):\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        if isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.constant_(m.bias, val=0.)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.constant_(m.bias, val=0.)\n",
    "        elif isinstance(m, nn.BatchNorm3d):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.constant_(m.bias, val=0.)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, val=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear FC Blocks\n",
    "def linear_block(n_inputs_loc, hidden_loc, \n",
    "                 activ_config=None,batch_config=None,p_drop_loc=0.1): \n",
    "    \n",
    "    # Dictionary defining Block Architecture\n",
    "    BlockArchitecture=[]\n",
    "   \n",
    "    hidden_loc.insert(0,n_inputs_loc)\n",
    "  \n",
    "    if activ_config==None:\n",
    "        activ_config=repmat('no_activ',len(hidden_loc),1)\n",
    "    if batch_config==None:\n",
    "        batch_config=repmat('no_batch',len(hidden_loc),1)\n",
    "    #Block Layers List\n",
    "    for i in np.arange(len(hidden_loc)-1):\n",
    "        BlockArchitecture.append(('linear'+str(i+1),\n",
    "                                  nn.Linear(hidden_loc[i], hidden_loc[i+1])))\n",
    "        \n",
    "        if(activ_config[i]=='relu'):\n",
    "            BlockArchitecture.append(('relu'+str(i+1),nn.ReLU(inplace=True)))\n",
    "           \n",
    "        elif(activ_config[i]=='tanh'):\n",
    "            BlockArchitecture.append(('tanh'+str(i+1),nn.Tanh()))\n",
    "        elif(activ_config[i]=='relu6'):\n",
    "             BlockArchitecture.append(('relu6'+str(i+1),nn.ReLU6(inplace=True)))\n",
    "             \n",
    "        if(batch_config[i]=='batch'):\n",
    "            BlockArchitecture.append(('batch'+str(i+1),nn.BatchNorm1d( hidden_loc[i+1])))\n",
    "         \n",
    "        BlockArchitecture.append(('drop'+str(i+1),nn.Dropout(p_drop_loc)))  \n",
    "    linear_block_loc = nn.Sequential(\n",
    "        OrderedDict(BlockArchitecture)\n",
    "        )\n",
    "    return linear_block_loc\n",
    "\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    MultiLayer Perceptron: \n",
    "    Netwotk with n_hidden layers with architecture linear+drop+relu+batch\n",
    "     Constructor Parameters:\n",
    "           n_inputs: dimensionality of input features (n_channels * n_features , by default) \n",
    "                     n_channels (=14), number of sensors or images for each case\n",
    "                     n_features(=40), number of features for each n_channels\n",
    "           n_classes: number of output classes (=3, by default)\n",
    "           hidden(=[128,128], default): list with the number of neurons for each hidden layer\n",
    "           p_drop(=0.1, default): probability for Drop layer (=0, no drop is performed)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inputmodule_params,net_params):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "       \n",
    "        ### Input Parameters\n",
    "        self.n_inputs = inputmodule_params['n_inputs']\n",
    "\n",
    "       \n",
    "        self.hidden=net_params['hidden']\n",
    "        self.dropout=net_params['dropout']\n",
    "        if net_params['dropout'] is None:\n",
    "            self.dropout=0.5\n",
    "        self.nlayers=len(self.hidden)\n",
    "        if 'activ_config' not in list(net_params.keys()):\n",
    "    \n",
    "            self.activ_config=None\n",
    "        else:\n",
    "             self.activ_config=net_params['activ_config']\n",
    "        \n",
    "        if 'batch_config' not in list(net_params.keys()):\n",
    "            self.batch_config=None\n",
    "        else:\n",
    "            self.batch_config=net_params['batch_config']\n",
    "             \n",
    "              \n",
    "        \n",
    "        self.linear_block0= linear_block(self.n_inputs, self.hidden.copy(), \n",
    "                                                 activ_config=self.activ_config, \n",
    "                                                 batch_config=self.batch_config,\n",
    "                                                 p_drop_loc=self.dropout)\n",
    "\n",
    "       \n",
    "        \n",
    "      #  self.fc_out=nn.Identity()\n",
    "        # weight init\n",
    "        init_weights_xavier_normal(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "              \n",
    "      \n",
    "        return self.linear_block0(x)\n",
    "\n",
    "### Convolutional \n",
    "class _CNNLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_input_features: int, n_neurons: int, kernel_sze:int =3, \n",
    "        stride:int=1,\n",
    "        drop_rate: float=0,\n",
    "        Relu=True\n",
    "         ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        norm1 = nn.BatchNorm2d(n_neurons)\n",
    "        conv1 = nn.Conv2d(num_input_features, n_neurons, kernel_size=kernel_sze,  \n",
    "                               stride=stride, padding=(int((kernel_sze-1)/2)))\n",
    "\n",
    "      #  relu1 = nn.ReLU(inplace=True)\n",
    "        relu1= nn.LeakyReLU(inplace=True)\n",
    "\n",
    "        drop=nn.Dropout(drop_rate)\n",
    "        if Relu:\n",
    "            self.cnn_layer=nn.Sequential(conv1,norm1,relu1,drop)\n",
    "        else:\n",
    "            self.cnn_layer=nn.Sequential(conv1,norm1,drop)\n",
    "        init_weights_xavier_normal(self)\n",
    "        \n",
    "    def forward(self, x: Tensor):\n",
    "         \n",
    "        return(self.cnn_layer(x))\n",
    "\n",
    "class _UnCNNLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_input_features: int, n_neurons: int, kernel_sze:int =3, \n",
    "        stride:int=2,\n",
    "        drop_rate: float=0, \n",
    "        Relu=True\n",
    "         ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stride=stride\n",
    "        norm1 = nn.BatchNorm2d(n_neurons)\n",
    "        conv1 = nn.ConvTranspose2d(num_input_features, n_neurons, kernel_size=kernel_sze,  \n",
    "                               stride=stride, padding=(int((kernel_sze-1)/2)))\n",
    "\n",
    "        \n",
    "     #   relu1 = nn.ReLU(inplace=True)\n",
    "        relu1 = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "        drop=nn.Dropout(drop_rate)\n",
    "        if Relu:\n",
    "            self.cnn_layer=nn.Sequential(conv1,norm1,relu1,drop)\n",
    "        else:\n",
    "            self.cnn_layer=nn.Sequential(conv1,norm1,drop)\n",
    "        init_weights_xavier_normal(self)\n",
    "        \n",
    "    def forward(self, x: Tensor):\n",
    "        \n",
    "        if  self.stride>1:\n",
    "            sze_enc=x.shape[-1]\n",
    "            x=self.cnn_layer[0](x,output_size=(sze_enc*2,sze_enc*2))\n",
    "            for k in np.arange(1,len(self.cnn_layer)):\n",
    "                x=self.cnn_layer[k](x)\n",
    "        else:\n",
    "            x=self.cnn_layer(x)\n",
    "            \n",
    "        return(x)\n",
    "    \n",
    "    # def forward(self, x1, x2):\n",
    "    #     x1 = self.cnn_layer(x1)\n",
    "    #     # input is CHW\n",
    "    #     diffY = x2.size()[2] - x1.size()[2]\n",
    "    #     diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "    #     x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "    #                     diffY // 2, diffY - diffY // 2])\n",
    "    #     # if you have padding issues, see\n",
    "    #     # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "    #     # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "    #     x = torch.cat([x2, x1], dim=1)\n",
    "    #     return self.conv(x)\n",
    "    \n",
    "class _CNNBlock(nn.ModuleDict):\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_channels: int=1,\n",
    "        drop_rate=0,\n",
    "        block_config = (64,128),\n",
    "        stride=None,\n",
    "        decoder=False,\n",
    "        Relu=True\n",
    "    \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        num_layers=len(block_config)\n",
    "        self.num_input_channels=num_input_channels\n",
    "        print('block inp ch',num_input_channels)\n",
    "        \n",
    "        if stride is None:\n",
    "            stride=np.ones(num_layers)\n",
    "            \n",
    "        for i in range(num_layers):\n",
    "            if decoder==True:\n",
    "                layer = _UnCNNLayer(\n",
    "                    num_input_channels,\n",
    "                    n_neurons=block_config[i],\n",
    "                    stride=stride[i],\n",
    "                    drop_rate=drop_rate\n",
    "                    \n",
    "                )\n",
    "            else:\n",
    "                layer = _CNNLayer(\n",
    "                    num_input_channels,\n",
    "                    n_neurons=block_config[i],\n",
    "                    stride=stride[i],\n",
    "                    drop_rate=drop_rate, \n",
    "                    Relu=Relu\n",
    "                    \n",
    "                )\n",
    "            self.add_module(\"cnnlayer%d\" % (i + 1), layer)\n",
    "            num_input_channels=block_config[i]\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \n",
    "        for name, layer in self.items():\n",
    "            x = layer(x)\n",
    "            \n",
    "            \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  BACKBONE MODULES \n",
    "class Encoder(nn.Module):\n",
    "    r\"\"\"Encoder class\n",
    "    `\".\n",
    "    Input Parameters:\n",
    "        1. inputmodule_params: dictionary with keys ['num_input_channels']\n",
    "            inputmodule_params['num_input_channels']=Channels of input images\n",
    "        2. net_params: dictionary defining architecture: \n",
    "            net_params['block_configs']: list of number of neurons for each \n",
    "            convolutional block. A block can have more than one layer\n",
    "            net_params['stride']:list of strides for each block layers\n",
    "            net_params['drop_rate']: value of the Dropout (equal for all blocks)\n",
    "        Examples: \n",
    "            1. Encoder with 4 blocks with one layer each\n",
    "            net_params['block_configs']=[[32],[64],[128],[256]]\n",
    "            net_params['stride']=[[2],[2],[2],[2]]\n",
    "            2. Encoder with 2 blocks with two layers each\n",
    "            net_params['block_configs']=[[32,32],[64,64]]\n",
    "            net_params['stride']=[[1,2],[1,2]]\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputmodule_params,net_params):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        num_input_channels=inputmodule_params['num_input_channels']\n",
    "        \n",
    "\n",
    "            \n",
    "        drop_rate=net_params['drop_rate']\n",
    "        block_configs=net_params['block_configs'].copy()\n",
    "        n_blocks=len(block_configs)\n",
    "        if 'stride' in net_params.keys():\n",
    "            stride=net_params['stride']\n",
    "        else:\n",
    "            stride=[]\n",
    "            for i in np.arange(len(block_configs)):\n",
    "                stride.append(list(np.ones(len(block_configs[i])-1,dtype=int))+[2])\n",
    "                \n",
    "        # Encoder\n",
    "        self.encoder=nn.Sequential(          \n",
    "            )\n",
    "        outchannels_encoder=[]\n",
    "        for i in np.arange(n_blocks):\n",
    "            print('block',i)\n",
    "            block = _CNNBlock(\n",
    "                num_input_channels=num_input_channels,\n",
    "                drop_rate=drop_rate,\n",
    "                block_config=block_configs[i],\n",
    "                stride= stride[i]               \n",
    "                \n",
    "            )\n",
    "            self.encoder.add_module(\"cnnblock%d\" % (i + 1), block)\n",
    "            \n",
    "            if stride==1:\n",
    "                self.encoder.add_module(\"mxpool%d\" % (i + 1), \n",
    "                                         nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
    "\n",
    "            num_input_channels=block_configs[i][-1] \n",
    "           # outchannels_encoder.append(num_input_channels)\n",
    "           \n",
    "          \n",
    "               \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \n",
    "        x=self.encoder(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    r\"\"\"Decoder class\n",
    "    `\".\n",
    "    Input Parameters:\n",
    "        1. inputmodule_params: dictionary with keys ['num_input_channels']\n",
    "            inputmodule_params['num_input_channels']=Channels of input images\n",
    "        2. net_params: dictionary defining architecture: \n",
    "            net_params['block_configs']: list of number of neurons for each conv block\n",
    "            net_params['stride']:list of strides for each block layers\n",
    "            net_params['drop_rate']: value of the Dropout (equal for all blocks)\n",
    "    \"\"\"\n",
    "    def __init__(self, inputmodule_params,net_params):\n",
    "        super().__init__()\n",
    "        \n",
    "   \n",
    "        num_input_channels=inputmodule_params['num_input_channels']\n",
    "        \n",
    "        self.upPoolMode='bilinear'\n",
    "\n",
    "            \n",
    "        drop_rate=net_params['drop_rate']\n",
    "        block_configs=net_params['block_configs'].copy()\n",
    "        self.n_blocks=len(block_configs)\n",
    "        \n",
    "        if 'stride' in net_params.keys():\n",
    "            stride=net_params['stride']\n",
    "        else:\n",
    "            stride=[]\n",
    "            for i in np.arange(len(block_configs)):\n",
    "                stride.append(list(np.ones(len(block_configs[i])-1,dtype=int))+[2])\n",
    "                \n",
    "\n",
    "        # Decoder\n",
    "        self.decoder=nn.Sequential(          \n",
    "            )\n",
    "        \n",
    "        for i0 in np.arange(self.n_blocks)[::-1]:\n",
    "            i=self.n_blocks-(i0+1)\n",
    "            block = _CNNBlock(\n",
    "                num_input_channels=num_input_channels,\n",
    "                drop_rate=drop_rate,\n",
    "                block_config=block_configs[i], \n",
    "                stride=stride[i],\n",
    "                decoder=True\n",
    "            )\n",
    "            \n",
    "            # if stride==1:\n",
    "            #     self.decoder.add_module(\"uppool%d\" % (i + 1), \n",
    "            #                               nn.Upsample(scale_factor=2, \n",
    "            #                                           mode=self.upPoolMode, align_corners=True))\n",
    "            \n",
    "            self.decoder.add_module(\"cnnblock%d\" % (i0+1), block)\n",
    "      \n",
    "\n",
    "            num_input_channels=block_configs[i][-1]\n",
    "        \n",
    "        \n",
    "        self.decoder[-1][list(self.decoder[-1].keys())[-1]].cnn_layer[2]=nn.Identity()\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \n",
    "        input_sze=x.shape\n",
    "\n",
    "     #   for i in np.arange(n_blocks)[::-1]:\n",
    "            \n",
    "        x=self.decoder(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "##### GENERATIVE MODELS \n",
    "class AutoEncoderCNN(nn.Module):\n",
    "    r\"\"\"AutoEncoderCNN model class\n",
    "    `\".\n",
    "    Input Parameters:\n",
    "        1. inputmodule_paramsEnc: dictionary with keys ['num_input_channels']\n",
    "            inputmodule_paramsEnc['num_input_channels']=Channels of input images\n",
    "        2. net_paramsEnc: dictionary defining architecture of the Encoder (see Encoder class) \n",
    "        3. inputmodule_paramsDec: dictionary with keys ['num_input_channels']\n",
    "           inputmodule_paramsDec['num_input_channels']=Channels of input images\n",
    "        4. net_paramsDec: dictionary defining architecture of the Encoder (see Decoder/Encoder classes) \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputmodule_paramsEnc,net_paramsEnc,inputmodule_paramsDec,net_paramsDec):\n",
    "        super().__init__()\n",
    " \n",
    "        self.inputmodule_paramsEnc=inputmodule_paramsEnc\n",
    "        self.inputmodule_paramsDec=inputmodule_paramsDec\n",
    "        self.net_paramsEnc=net_paramsEnc\n",
    "        self.net_paramsDec=net_paramsDec\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder=Encoder(inputmodule_paramsEnc,net_paramsEnc)\n",
    "     \n",
    "        # Decoder\n",
    "        self.decoder=Decoder(inputmodule_paramsDec,net_paramsDec)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "            # Guardamos el tamaño de la imagen de entrada para usarlo al final (en la fase de upsampling)\n",
    "            input_size = x.shape\n",
    "\n",
    "            # Paso por el Encoder\n",
    "            encoded = self.encoder(x)\n",
    "            \n",
    "            # Paso por el Decoder\n",
    "            decoded = self.decoder(encoded)\n",
    "            \n",
    "            # Upsampling a las dimensiones originales de la imagen de entrada\n",
    "            output = F.interpolate(decoded, size=input_size[2:], mode=self.upPoolMode)\n",
    "            \n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B22-01\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Filtrando imágenes y etiquetas para obtener solo aquellas cuya densidad es 0\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_cropped\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m filtered_images \u001b[38;5;241m=\u001b[39m [img \u001b[38;5;28;01mfor\u001b[39;00m img, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(images, labels) \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m filtered_labels \u001b[38;5;241m=\u001b[39m [label \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[21], line 21\u001b[0m, in \u001b[0;36mload_cropped\u001b[1;34m(folder_path, csv_path, patient_list, sample_size)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(patient_id)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatient_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_*\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m---> 21\u001b[0m patient_folder \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpatient_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Verificar que el paciente esté en el CSV\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m patient_id \u001b[38;5;129;01min\u001b[39;00m patient_metadata:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Obtener todas las imágenes .png dentro de la carpeta del paciente\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Filtrando imágenes y etiquetas para obtener solo aquellas cuya densidad es 0\n",
    "images, labels = load_cropped(folder_path, csv_path, patient_list, sample_size)\n",
    "filtered_images = [img for img, label in zip(images, labels) if label == 0]\n",
    "filtered_labels = [label for label in labels if label == 0]\n",
    "\n",
    "# Crear el Dataset y DataLoader solo con las imágenes de densidad 0\n",
    "filtered_dataset_train = Standard_Dataset(filtered_images)\n",
    "dataloader_train = DataLoader(filtered_dataset_train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block 0\n",
      "block inp ch 3\n",
      "block 1\n",
      "block inp ch 32\n",
      "block inp ch 64\n",
      "block inp ch 32\n"
     ]
    }
   ],
   "source": [
    "def AEConfigs(Config):\n",
    "    \n",
    "    if Config=='1':\n",
    "        # CONFIG1\n",
    "        net_paramsEnc['block_configs']=[[32,32],[64,64]]\n",
    "        net_paramsEnc['stride']=[[1,2],[1,2]]\n",
    "        net_paramsEnc['drop_rate']=0.1\n",
    "        net_paramsDec['drop_rate']=0.1\n",
    "        net_paramsDec['block_configs']=[[64,32],[32,inputmodule_paramsEnc['num_input_channels']]]\n",
    "        net_paramsDec['stride']=net_paramsEnc['stride']\n",
    "        inputmodule_paramsDec['num_input_channels']=net_paramsEnc['block_configs'][-1][-1]\n",
    "     \n",
    "\n",
    "        \n",
    "    elif Config=='2':\n",
    "        # CONFIG 2\n",
    "        net_paramsEnc['block_configs']=[[32],[64],[128],[256]]\n",
    "        net_paramsEnc['stride']=[[2],[2],[2],[2]]\n",
    "        net_paramsDec['block_configs']=[[128],[64],[32],[inputmodule_paramsEnc['num_input_channels']]]\n",
    "        net_paramsDec['stride']=net_paramsEnc['stride']\n",
    "        inputmodule_paramsDec['num_input_channels']=net_paramsEnc['block_configs'][-1][-1]\n",
    "   \n",
    "        \n",
    "    elif Config=='3':  \n",
    "        # CONFIG3\n",
    "        net_paramsEnc['block_configs']=[[32],[64],[64]]\n",
    "        net_paramsEnc['stride']=[[1],[2],[2]]\n",
    "        net_paramsDec['block_configs']=[[64],[32],[inputmodule_paramsEnc['num_input_channels']]]\n",
    "        net_paramsDec['stride']=net_paramsEnc['stride']\n",
    "        inputmodule_paramsDec['num_input_channels']=net_paramsEnc['block_configs'][-1][-1]\n",
    "    \n",
    "    return net_paramsEnc,net_paramsDec,inputmodule_paramsDec\n",
    "\n",
    "\n",
    "######################### 0. EXPERIMENT PARAMETERS\n",
    "\n",
    "# 0.2 Parámetros y definiciones de red\n",
    "inputmodule_paramsEnc = {'num_input_channels': 3}\n",
    "net_paramsEnc = {}\n",
    "net_paramsDec = {}\n",
    "inputmodule_paramsDec = {}\n",
    "\n",
    "# 0.1 AE PARAMETERS\n",
    "inputmodule_paramsEnc={}\n",
    "inputmodule_paramsEnc['num_input_channels']=3\n",
    "\n",
    "###### CONFIG1\n",
    "Config='1'\n",
    "net_paramsEnc,net_paramsDec,inputmodule_paramsDec=AEConfigs(Config)\n",
    "model=AutoEncoderCNN(inputmodule_paramsEnc, net_paramsEnc,\n",
    "                     inputmodule_paramsDec, net_paramsDec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding\n",
      "decoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/13 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Paso forward\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Calcular la pérdida\u001b[39;00m\n\u001b[0;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, images)\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 177\u001b[0m, in \u001b[0;36mAutoEncoderCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Paso por el Decoder\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoding\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Upsampling a las dimensiones originales de la imagen de entrada\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 136\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    132\u001b[0m    input_sze\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m#   for i in np.arange(n_blocks)[::-1]:\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m    x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 220\u001b[0m, in \u001b[0;36m_CNNBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 220\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 156\u001b[0m, in \u001b[0;36m_UnCNNLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    154\u001b[0m         x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn_layer[k](x)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(x)\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mirvi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Hiperparámetros del entrenamiento\n",
    "# num_epochs = 10  # Número de épocas de entrenamiento\n",
    "# learning_rate = 0.001  # Tasa de aprendizaje\n",
    "\n",
    "# # Definir el optimizador (en este caso, Adam)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Definir la función de pérdida (MSE)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # Enviar el modelo a la GPU si está disponible\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Bucle de entrenamiento\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()  # Poner el modelo en modo de entrenamiento\n",
    "#     running_loss = 0.0  # Para almacenar la pérdida de cada época\n",
    "\n",
    "#     # Iterar sobre el dataloader\n",
    "#     for batch_idx, images in enumerate(tqdm(dataloader_train, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "#         # Enviar los datos a la GPU si está disponible\n",
    "#         images=images.permute(0,3,1,2) # permute images\n",
    "#         images = images.to(device)\n",
    "        \n",
    "#         # Inicializar el gradiente a cero\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Paso forward\n",
    "#         outputs = model(images)\n",
    "        \n",
    "#         # Calcular la pérdida\n",
    "#         loss = criterion(outputs, images)\n",
    "        \n",
    "#         # Paso backward (retropropagación)\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # Actualizar los pesos\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # Acumular la pérdida\n",
    "#         running_loss += loss.item()\n",
    "    \n",
    "#     # Imprimir la pérdida promedio de cada época\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, images in enumerate(dataloader_train):\n",
    "    print(images.shape)  # Añadir esta línea para ver la forma de las imágenes\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
