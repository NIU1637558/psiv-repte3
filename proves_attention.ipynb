{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "#### Simple Neural Network          \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, net_params):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Get network parameters\n",
    "        in_dimension = net_params['in_features']\n",
    "        out_dimension = net_params['out_features']\n",
    "       \n",
    "        self.net=nn.Sequential(nn.Linear(in_dimension, round(in_dimension / 2)),  \n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Dropout(inplace=False, p=0.5),\n",
    "                            nn.Linear(round(in_dimension / 2), out_dimension))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "#### Attention Neural Netwrok\n",
    "# Attention Units\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,net_params):\n",
    "        super(Attention, self).__init__()\n",
    "        self.M = net_params['in_features'] #Input dimension of the Values NV vectors \n",
    "        self.L = net_params['decom_space'] # Dimension of Q(uery),K(eys) decomposition space\n",
    "        self.ATTENTION_BRANCHES = net_params['ATTENTION_BRANCHES']\n",
    "\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.M, self.L), # matrix V\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.L, self.ATTENTION_BRANCHES) # matrix w (or vector w if self.ATTENTION_BRANCHES==1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # H feature vector matrix  # NV vectors x M dimensions\n",
    "        H = x.squeeze(0)\n",
    "        # Attention weights\n",
    "        A = self.attention(H)  # NVxATTENTION_BRANCHES\n",
    "        A = torch.transpose(A, 1, 0)  # ATTENTION_BRANCHESxNV\n",
    "        A = F.softmax(A, dim=1)  # softmax over NV\n",
    "        \n",
    "        # Context Vector (Attention Aggregation)\n",
    "        Z = torch.mm(A, H)  # ATTENTION_BRANCHESxM \n",
    "        \n",
    "        return Z, A\n",
    "\n",
    "\n",
    "class GatedAttention(nn.Module):\n",
    "    def __init__(self,net_params):\n",
    "        super(GatedAttention, self).__init__()\n",
    "        self.M = net_params['in_features'] #Input dimension of the Values NV vectors \n",
    "        self.L = net_params['decom_space'] # Dimension of Q(uery),K(eys) decomposition space\n",
    "        self.ATTENTION_BRANCHES = net_params['ATTENTION_BRANCHES']\n",
    "        \n",
    "        # Matrix for Query decomposition\n",
    "        self.attention_V = nn.Sequential(\n",
    "            nn.Linear(self.M, self.L), # matrix V\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # Matrix for Keys decomposition\n",
    "        self.attention_U = nn.Sequential(\n",
    "            nn.Linear(self.M, self.L), # matrix U\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.attention_w = nn.Linear(self.L, self.ATTENTION_BRANCHES) # matrix w (or vector w if self.ATTENTION_BRANCHES==1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # H feature vector matrix  # NV vectors x M dimensions\n",
    "        H = x.squeeze(0)\n",
    "        ## Self Attention weights\n",
    "        # Input Vector Query Decomposition, Q\n",
    "        A_V = self.attention_V(H)  # NVxL (Projecion of the V input vectors into L dim space)\n",
    "        # Input Vector Keys Decomposition, K\n",
    "        A_U = self.attention_U(H)  # NVxL\n",
    "        # Attention Matrix from Product Q*K \n",
    "        A = self.attention_w(A_V * A_U) # element wise multiplication # NVxATTENTION_BRANCHES\n",
    "        A = torch.transpose(A, 1, 0)  # ATTENTION_BRANCHESxNV\n",
    "        A = F.softmax(A, dim=1)  # softmax over NV dimension\n",
    "        \n",
    "        ## Context Vector (Attention Aggregation)\n",
    "        Z = torch.mm(A, H)  # ATTENTION_BRANCHESxM\n",
    "\n",
    "        return Z, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llibreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "import itertools\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from numpy.matlib import repmat\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "import cv2\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD CROPPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "\n",
    "def load_cropped(folder_path, csv_path, patient_list = [], sample_size=200):\n",
    "    # Cargar el CSV con pandas\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Convertir el CSV a un diccionario para un acceso rápido\n",
    "    patient_metadata = {row['CODI']: row['DENSITAT'] for _, row in df.iterrows()}\n",
    "\n",
    "    # Inicializar la estructura de datos para almacenar los datos de los pacientes seleccionados\n",
    "    patients_data = []\n",
    "    images_list = []\n",
    "\n",
    "    # si no se proporciona una lista de pacientes, se seleccionan todos los pacientes iterando folder_path\n",
    "    if not patient_list:\n",
    "        for patient_folder in glob.glob(os.path.join(folder_path, \"*\")):\n",
    "            patient_id = os.path.basename(patient_folder).split(\"_\")[0]\n",
    "            patient_list.append(patient_id)\n",
    "    \n",
    "    incorrect_shape = 0\n",
    "    # Iterar sobre cada paciente en la lista de IDs proporcionada\n",
    "    for i,patient_id in enumerate(patient_list):\n",
    "        print('--- Loading data from patient:', patient_id, f'(--- {i+1}/{len(patient_list)}) ---')\n",
    "        # Obtener carpeta del paciente\n",
    "        try:\n",
    "            patient_folder = glob.glob(os.path.join(folder_path, f\"{patient_id}_*\"))[0]\n",
    "        except:\n",
    "            print(f'Patient {patient_id} not found in the dataset folder')\n",
    "            continue\n",
    "\n",
    "        # Verificar que el paciente esté en el CSV\n",
    "        if patient_id in patient_metadata:\n",
    "            # Obtener todas las imágenes .png dentro de la carpeta del paciente\n",
    "            images = glob.glob(os.path.join(patient_folder, \"*.png\"))\n",
    "            \n",
    "            # Si el paciente tiene imágenes en su carpeta\n",
    "            if images:\n",
    "                # Mezclar la lista de imágenes\n",
    "                random.shuffle(images)\n",
    "                \n",
    "                # Seleccionar una muestra de tamaño sample_size o menos si hay menos imágenes\n",
    "                images_sampled = random.sample(images, min(sample_size, len(images)))\n",
    "                \n",
    "                for j,image_path in enumerate(images_sampled):\n",
    "                    if j % 100 == 0:\n",
    "                        print(f'------ Loading image {j}/{len(images_sampled)} ---')\n",
    "\n",
    "                    # Cargar la imagen en formato BGR con cv2\n",
    "                    image_bgr = cv2.imread(image_path)\n",
    "                    \n",
    "                    # Convertir la imagen de BGR a RGB\n",
    "                    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    if image_rgb.shape[0] != 256 or image_rgb.shape[1] != 256:\n",
    "                        incorrect_shape +=1\n",
    "                        # Resize de la imagen a 256x256\n",
    "                        image_rgb = cv2.resize(image_rgb, (256, 256))\n",
    "\n",
    "                    # Permutar canales\n",
    "                    image_rgb = np.transpose(image_rgb, (2, 0, 1))\n",
    "\n",
    "                    # Pasar de uint8 a float32\n",
    "                    image_rgb = image_rgb/255.0\n",
    "                    \n",
    "                    # Añadir la imagen a la lista de imágenes en formato RGB\n",
    "                    images_list.append(image_rgb)\n",
    "                # Binariar densidad\n",
    "                if patient_metadata[patient_id] == \"NEGATIVA\":\n",
    "                    dens = 0\n",
    "                else:\n",
    "                    dens = 1\n",
    "                \n",
    "                # Añadir la densidad a la lista de metadatos\n",
    "                patients_data.extend([dens] * len(images_sampled))\n",
    "\n",
    "    print(f'Resized images: {incorrect_shape}/{len(images_list)}')\n",
    "    \n",
    "    return images_list, patients_data\n",
    "\n",
    "# Paso 2: Crear la clase Standard_Dataset\n",
    "class Standard_Dataset(data.Dataset):\n",
    "    def __init__(self, X, Y=None, transformation=None):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        self.transformation = transformation\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]\n",
    "        if self.transformation:\n",
    "            image = self.transformation(image)\n",
    "            \n",
    "        image_tensor = torch.from_numpy(image).float()\n",
    "        \n",
    "        if self.y is not None:\n",
    "            label = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "            return image_tensor, label\n",
    "        else:\n",
    "            return image_tensor\n",
    "\n",
    "# Paso 3: Cargar los datos\n",
    "folder_path = \"C:/Users/mirvi/Desktop/mii/UAB/4.1/PSIV2/detect mateicules/repte3/psiv-repte3/data/Cropped_sample\"\n",
    "csv_path = \"C:/Users/mirvi/Desktop/mii/UAB/4.1/PSIV2/detect mateicules/repte3/psiv-repte3/data/PatientDiagnosis.csv\"\n",
    "patient_list = ['B22-25']\n",
    "sample_size = 1\n",
    "\n",
    "# Filtrando imágenes y etiquetas para obtener solo aquellas cuya densidad es 0\n",
    "images, labels = load_cropped(folder_path, csv_path, patient_list, sample_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOENCODER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def init_weights_transformer(module,InitParams):\n",
    "    Type=InitParams['Type']\n",
    "    U=InitParams['U']\n",
    "    \n",
    "    if Type=='ori':\n",
    "        init_weights_transformer_ori(module)\n",
    "    elif Type=='all':\n",
    "        init_weights_transformer_all(module,U)       \n",
    "    else:\n",
    "        init_weights_transformer_exceptnorm(module,U) \n",
    "    \n",
    "def init_weights_transformer_all(module,U=[0,1]):\n",
    "    param=module.state_dict()\n",
    "   \n",
    "    for name in param.keys():\n",
    "        if name.find('norm')<0:\n",
    "           nn.init.uniform_(param[name],a=U[0],b=U[1])\n",
    "\n",
    "                \n",
    "def init_weights_transformer_exceptnorm(module,U=[0,1]):\n",
    "    param=module.state_dict()\n",
    "   \n",
    "    for name in param.keys():\n",
    "        if name.find('norm')<0:\n",
    "           if (name.find('weight')>0):\n",
    "                nn.init.uniform_(param[name],a=U[0],b=U[1])\n",
    "           elif (name.find('bias')>0):\n",
    "                nn.init.constant_(param[name], 0)\n",
    "    #        \n",
    "def init_weights_transformer_ori(module):\n",
    "    initrange = 0.1\n",
    "    nn.init.uniform_(module.encoder.weight, -initrange, initrange)\n",
    "    nn.init.zeros_(module.decoder1[0].bias)\n",
    "    nn.init.zeros_(module.decoder2.bias)\n",
    "    nn.init.uniform_(module.decoder1[0].weight, -initrange, initrange)\n",
    "    nn.init.uniform_(module.decoder2.weight, -initrange, initrange)\n",
    "    \n",
    "\n",
    "def init_weights_xavier_normal(module):\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv3d):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm3d):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LSTM):\n",
    "            for param in m.parameters():\n",
    "                if len(param.shape) >= 2:\n",
    "                    nn.init.orthogonal_(param.data)\n",
    "                else:\n",
    "                    nn.init.normal_(param.data)\n",
    "\n",
    "def init_weights_xavier_uniform(module):\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv3d):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.uniform_(m.weight, a=0, b=1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.uniform_(m.weight, a=0, b=1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm3d):\n",
    "            nn.init.uniform_(m.weight, a=0, b=1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LSTM):\n",
    "            for param in m.parameters():\n",
    "                if len(param.shape) >= 2:\n",
    "                    nn.init.orthogonal_(param.data)\n",
    "                else:\n",
    "                    nn.init.uniform_(param.data)\n",
    "\n",
    "def init_weights_kaiming_uniform(module):\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.uniform_(m.weight, a=0, b=1)\n",
    "            nn.init.constant_(m.bias, val=0.)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.uniform_(m.weight, a=0, b=1)\n",
    "            nn.init.constant_(m.bias, val=0.)\n",
    "        elif isinstance(m, nn.BatchNorm3d):\n",
    "            nn.init.uniform_(m.weight, a=0, b=1)\n",
    "            nn.init.constant_(m.bias, val=0.)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, val=0.)\n",
    "\n",
    "def init_weights_kaiming_normal(module):\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        if isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.constant_(m.bias, val=0.)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.constant_(m.bias, val=0.)\n",
    "        elif isinstance(m, nn.BatchNorm3d):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.constant_(m.bias, val=0.)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, val=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear FC Blocks\n",
    "def linear_block(n_inputs_loc, hidden_loc, \n",
    "                 activ_config=None,batch_config=None,p_drop_loc=0.1): \n",
    "    \n",
    "    # Dictionary defining Block Architecture\n",
    "    BlockArchitecture=[]\n",
    "   \n",
    "    hidden_loc.insert(0,n_inputs_loc)\n",
    "  \n",
    "    if activ_config==None:\n",
    "        activ_config=repmat('no_activ',len(hidden_loc),1)\n",
    "    if batch_config==None:\n",
    "        batch_config=repmat('no_batch',len(hidden_loc),1)\n",
    "    #Block Layers List\n",
    "    for i in np.arange(len(hidden_loc)-1):\n",
    "        BlockArchitecture.append(('linear'+str(i+1),\n",
    "                                  nn.Linear(hidden_loc[i], hidden_loc[i+1])))\n",
    "        \n",
    "        if(activ_config[i]=='relu'):\n",
    "            BlockArchitecture.append(('relu'+str(i+1),nn.ReLU(inplace=True)))\n",
    "           \n",
    "        elif(activ_config[i]=='tanh'):\n",
    "            BlockArchitecture.append(('tanh'+str(i+1),nn.Tanh()))\n",
    "        elif(activ_config[i]=='relu6'):\n",
    "             BlockArchitecture.append(('relu6'+str(i+1),nn.ReLU6(inplace=True)))\n",
    "             \n",
    "        if(batch_config[i]=='batch'):\n",
    "            BlockArchitecture.append(('batch'+str(i+1),nn.BatchNorm1d( hidden_loc[i+1])))\n",
    "         \n",
    "        BlockArchitecture.append(('drop'+str(i+1),nn.Dropout(p_drop_loc)))  \n",
    "    linear_block_loc = nn.Sequential(\n",
    "        OrderedDict(BlockArchitecture)\n",
    "        )\n",
    "    return linear_block_loc\n",
    "\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    MultiLayer Perceptron: \n",
    "    Netwotk with n_hidden layers with architecture linear+drop+relu+batch\n",
    "     Constructor Parameters:\n",
    "           n_inputs: dimensionality of input features (n_channels * n_features , by default) \n",
    "                     n_channels (=14), number of sensors or images for each case\n",
    "                     n_features(=40), number of features for each n_channels\n",
    "           n_classes: number of output classes (=3, by default)\n",
    "           hidden(=[128,128], default): list with the number of neurons for each hidden layer\n",
    "           p_drop(=0.1, default): probability for Drop layer (=0, no drop is performed)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inputmodule_params,net_params):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "       \n",
    "        ### Input Parameters\n",
    "        self.n_inputs = inputmodule_params['n_inputs']\n",
    "\n",
    "       \n",
    "        self.hidden=net_params['hidden']\n",
    "        self.dropout=net_params['dropout']\n",
    "        if net_params['dropout'] is None:\n",
    "            self.dropout=0.5\n",
    "        self.nlayers=len(self.hidden)\n",
    "        if 'activ_config' not in list(net_params.keys()):\n",
    "    \n",
    "            self.activ_config=None\n",
    "        else:\n",
    "             self.activ_config=net_params['activ_config']\n",
    "        \n",
    "        if 'batch_config' not in list(net_params.keys()):\n",
    "            self.batch_config=None\n",
    "        else:\n",
    "            self.batch_config=net_params['batch_config']\n",
    "             \n",
    "              \n",
    "        \n",
    "        self.linear_block0= linear_block(self.n_inputs, self.hidden.copy(), \n",
    "                                                 activ_config=self.activ_config, \n",
    "                                                 batch_config=self.batch_config,\n",
    "                                                 p_drop_loc=self.dropout)\n",
    "\n",
    "       \n",
    "        \n",
    "      #  self.fc_out=nn.Identity()\n",
    "        # weight init\n",
    "        init_weights_xavier_normal(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "              \n",
    "      \n",
    "        return self.linear_block0(x)\n",
    "\n",
    "### Convolutional \n",
    "class _CNNLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_input_features: int, n_neurons: int, kernel_sze:int =3, \n",
    "        stride:int=1,\n",
    "        drop_rate: float=0,\n",
    "        Relu=True\n",
    "         ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        norm1 = nn.BatchNorm2d(n_neurons)\n",
    "        conv1 = nn.Conv2d(num_input_features, n_neurons, kernel_size=kernel_sze,  \n",
    "                               stride=stride, padding=(int((kernel_sze-1)/2)))\n",
    "\n",
    "      #  relu1 = nn.ReLU(inplace=True)\n",
    "        relu1= nn.LeakyReLU(inplace=True)\n",
    "\n",
    "        drop=nn.Dropout(drop_rate)\n",
    "        if Relu:\n",
    "            self.cnn_layer=nn.Sequential(conv1,norm1,relu1,drop)\n",
    "        else:\n",
    "            self.cnn_layer=nn.Sequential(conv1,norm1,drop)\n",
    "        init_weights_xavier_normal(self)\n",
    "        \n",
    "    def forward(self, x: Tensor):\n",
    "         \n",
    "        return(self.cnn_layer(x))\n",
    "\n",
    "class _UnCNNLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_input_features: int, n_neurons: int, kernel_sze:int =3, \n",
    "        stride:int=2,\n",
    "        drop_rate: float=0, \n",
    "        Relu=True\n",
    "         ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stride=stride\n",
    "        norm1 = nn.BatchNorm2d(n_neurons)\n",
    "        conv1 = nn.ConvTranspose2d(num_input_features, n_neurons, kernel_size=kernel_sze,  \n",
    "                               stride=stride, padding=(int((kernel_sze-1)/2)))\n",
    "\n",
    "        \n",
    "     #   relu1 = nn.ReLU(inplace=True)\n",
    "        relu1 = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "        drop=nn.Dropout(drop_rate)\n",
    "        if Relu:\n",
    "            self.cnn_layer=nn.Sequential(conv1,norm1,relu1,drop)\n",
    "        else:\n",
    "            self.cnn_layer=nn.Sequential(conv1,norm1,drop)\n",
    "        init_weights_xavier_normal(self)\n",
    "        \n",
    "    def forward(self, x: Tensor):\n",
    "        \n",
    "        if  self.stride>1:\n",
    "            sze_enc=x.shape[-1]\n",
    "            x=self.cnn_layer[0](x,output_size=(sze_enc*2,sze_enc*2))\n",
    "            for k in np.arange(1,len(self.cnn_layer)):\n",
    "                x=self.cnn_layer[k](x)\n",
    "        else:\n",
    "            x=self.cnn_layer(x)\n",
    "            \n",
    "        return(x)\n",
    "    \n",
    "    # def forward(self, x1, x2):\n",
    "    #     x1 = self.cnn_layer(x1)\n",
    "    #     # input is CHW\n",
    "    #     diffY = x2.size()[2] - x1.size()[2]\n",
    "    #     diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "    #     x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "    #                     diffY // 2, diffY - diffY // 2])\n",
    "    #     # if you have padding issues, see\n",
    "    #     # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "    #     # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "    #     x = torch.cat([x2, x1], dim=1)\n",
    "    #     return self.conv(x)\n",
    "    \n",
    "class _CNNBlock(nn.ModuleDict):\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_channels: int=1,\n",
    "        drop_rate=0,\n",
    "        block_config = (64,128),\n",
    "        stride=None,\n",
    "        decoder=False,\n",
    "        Relu=True\n",
    "    \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        num_layers=len(block_config)\n",
    "        self.num_input_channels=num_input_channels\n",
    "        print('block inp ch',num_input_channels)\n",
    "        \n",
    "        if stride is None:\n",
    "            stride=np.ones(num_layers)\n",
    "            \n",
    "        for i in range(num_layers):\n",
    "            if decoder==True:\n",
    "                layer = _UnCNNLayer(\n",
    "                    num_input_channels,\n",
    "                    n_neurons=block_config[i],\n",
    "                    stride=stride[i],\n",
    "                    drop_rate=drop_rate\n",
    "                    \n",
    "                )\n",
    "            else:\n",
    "                layer = _CNNLayer(\n",
    "                    num_input_channels,\n",
    "                    n_neurons=block_config[i],\n",
    "                    stride=stride[i],\n",
    "                    drop_rate=drop_rate, \n",
    "                    Relu=Relu\n",
    "                    \n",
    "                )\n",
    "            self.add_module(\"cnnlayer%d\" % (i + 1), layer)\n",
    "            num_input_channels=block_config[i]\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \n",
    "        for name, layer in self.items():\n",
    "            x = layer(x)\n",
    "            \n",
    "            \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  BACKBONE MODULES \n",
    "class Encoder(nn.Module):\n",
    "    r\"\"\"Encoder class\n",
    "    `\".\n",
    "    Input Parameters:\n",
    "        1. inputmodule_params: dictionary with keys ['num_input_channels']\n",
    "            inputmodule_params['num_input_channels']=Channels of input images\n",
    "        2. net_params: dictionary defining architecture: \n",
    "            net_params['block_configs']: list of number of neurons for each \n",
    "            convolutional block. A block can have more than one layer\n",
    "            net_params['stride']:list of strides for each block layers\n",
    "            net_params['drop_rate']: value of the Dropout (equal for all blocks)\n",
    "        Examples: \n",
    "            1. Encoder with 4 blocks with one layer each\n",
    "            net_params['block_configs']=[[32],[64],[128],[256]]\n",
    "            net_params['stride']=[[2],[2],[2],[2]]\n",
    "            2. Encoder with 2 blocks with two layers each\n",
    "            net_params['block_configs']=[[32,32],[64,64]]\n",
    "            net_params['stride']=[[1,2],[1,2]]\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputmodule_params,net_params):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        num_input_channels=inputmodule_params['num_input_channels']\n",
    "        \n",
    "\n",
    "            \n",
    "        drop_rate=net_params['drop_rate']\n",
    "        block_configs=net_params['block_configs'].copy()\n",
    "        n_blocks=len(block_configs)\n",
    "        if 'stride' in net_params.keys():\n",
    "            stride=net_params['stride']\n",
    "        else:\n",
    "            stride=[]\n",
    "            for i in np.arange(len(block_configs)):\n",
    "                stride.append(list(np.ones(len(block_configs[i])-1,dtype=int))+[2])\n",
    "                \n",
    "        # Encoder\n",
    "        self.encoder=nn.Sequential(          \n",
    "            )\n",
    "        outchannels_encoder=[]\n",
    "        for i in np.arange(n_blocks):\n",
    "            print('block',i)\n",
    "            block = _CNNBlock(\n",
    "                num_input_channels=num_input_channels,\n",
    "                drop_rate=drop_rate,\n",
    "                block_config=block_configs[i],\n",
    "                stride= stride[i]               \n",
    "                \n",
    "            )\n",
    "            self.encoder.add_module(\"cnnblock%d\" % (i + 1), block)\n",
    "            \n",
    "            if stride==1:\n",
    "                self.encoder.add_module(\"mxpool%d\" % (i + 1), \n",
    "                                         nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
    "\n",
    "            num_input_channels=block_configs[i][-1] \n",
    "           # outchannels_encoder.append(num_input_channels)\n",
    "                 \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x=self.encoder(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    r\"\"\"Decoder class\n",
    "    `\".\n",
    "    Input Parameters:\n",
    "        1. inputmodule_params: dictionary with keys ['num_input_channels']\n",
    "            inputmodule_params['num_input_channels']=Channels of input images\n",
    "        2. net_params: dictionary defining architecture: \n",
    "            net_params['block_configs']: list of number of neurons for each conv block\n",
    "            net_params['stride']:list of strides for each block layers\n",
    "            net_params['drop_rate']: value of the Dropout (equal for all blocks)\n",
    "    \"\"\"\n",
    "    def __init__(self, inputmodule_params,net_params):\n",
    "        super().__init__()\n",
    "        \n",
    "   \n",
    "        num_input_channels=inputmodule_params['num_input_channels']\n",
    "        \n",
    "        self.upPoolMode='bilinear'\n",
    "\n",
    "            \n",
    "        drop_rate=net_params['drop_rate']\n",
    "        block_configs=net_params['block_configs'].copy()\n",
    "        self.n_blocks=len(block_configs)\n",
    "        \n",
    "        if 'stride' in net_params.keys():\n",
    "            stride=net_params['stride']\n",
    "        else:\n",
    "            stride=[]\n",
    "            for i in np.arange(len(block_configs)):\n",
    "                stride.append(list(np.ones(len(block_configs[i])-1,dtype=int))+[2])\n",
    "                \n",
    "\n",
    "        # Decoder\n",
    "        self.decoder=nn.Sequential(          \n",
    "            )\n",
    "        \n",
    "        for i0 in np.arange(self.n_blocks)[::-1]:\n",
    "            i=self.n_blocks-(i0+1)\n",
    "            block = _CNNBlock(\n",
    "                num_input_channels=num_input_channels,\n",
    "                drop_rate=drop_rate,\n",
    "                block_config=block_configs[i], \n",
    "                stride=stride[i],\n",
    "                decoder=True\n",
    "            )\n",
    "            \n",
    "            # if stride==1:\n",
    "            #     self.decoder.add_module(\"uppool%d\" % (i + 1), \n",
    "            #                               nn.Upsample(scale_factor=2, \n",
    "            #                                           mode=self.upPoolMode, align_corners=True))\n",
    "            \n",
    "            self.decoder.add_module(\"cnnblock%d\" % (i0+1), block)\n",
    "      \n",
    "\n",
    "            num_input_channels=block_configs[i][-1]\n",
    "        \n",
    "        \n",
    "        self.decoder[-1][list(self.decoder[-1].keys())[-1]].cnn_layer[2]=nn.Identity()\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        input_sze=x.shape\n",
    "     #   for i in np.arange(n_blocks)[::-1]:     \n",
    "        x=self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,net_params):\n",
    "        super(Attention, self).__init__()\n",
    "        self.M = net_params['in_features'] #Input dimension of the Values NV vectors \n",
    "        self.L = net_params['decom_space'] # Dimension of Q(uery),K(eys) decomposition space\n",
    "        self.ATTENTION_BRANCHES = net_params['ATTENTION_BRANCHES']\n",
    "\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.M, self.L), # matrix V\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.L, self.ATTENTION_BRANCHES) # matrix w (or vector w if self.ATTENTION_BRANCHES==1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "\n",
    "        # H feature vector matrix  # NV vectors x M dimensions\n",
    "        H = x.squeeze(0)\n",
    "        # Attention weights\n",
    "        A = self.attention(H)  # NVxATTENTION_BRANCHES\n",
    "        A = torch.transpose(A, 1, 0)  # ATTENTION_BRANCHESxNV\n",
    "        A = F.softmax(A, dim=1)  # softmax over NV\n",
    "        \n",
    "        # Context Vector (Attention Aggregation)\n",
    "        Z = torch.mm(A, H)  # ATTENTION_BRANCHESxM \n",
    "        \n",
    "        return Z, A\n",
    "\n",
    "##### GENERATIVE MODELS \n",
    "class AutoEncoderWithAttention(nn.Module):\n",
    "    r\"\"\"AutoEncoderCNN model with Attention.\n",
    "    Incorporates an attention mechanism between the encoder and decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputmodule_paramsEnc, net_paramsEnc, inputmodule_paramsDec, net_paramsDec, attention_params):\n",
    "        super().__init__()\n",
    "\n",
    "        self.inputmodule_paramsEnc = inputmodule_paramsEnc\n",
    "        self.inputmodule_paramsDec = inputmodule_paramsDec\n",
    "        self.net_paramsEnc = net_paramsEnc\n",
    "        self.net_paramsDec = net_paramsDec\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(inputmodule_paramsEnc, net_paramsEnc)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = Attention(attention_params)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(inputmodule_paramsDec, net_paramsDec)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Paso por el Encoder\n",
    "        encoded = self.encoder(x)\n",
    "\n",
    "        # Aplicamos la atención al cuello de botella\n",
    "        attended, attention_weights = self.attention(encoded)\n",
    "\n",
    "        # Paso por el Decoder\n",
    "        decoded = self.decoder(attended.unsqueeze(0))  # Añadimos la dimensión de batch\n",
    "        return decoded, attention_weights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
